{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d6ae259",
   "metadata": {
    "papermill": {
     "duration": 0.015072,
     "end_time": "2024-05-26T09:49:36.733561",
     "exception": false,
     "start_time": "2024-05-26T09:49:36.718489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Home Credit  Risk Model Pipeline\n",
    "\n",
    "Welcome to our advanced pipeline for building and assessing credit risk models using machine learning! ðŸ“ŠðŸ’³\n",
    "\n",
    "## Overview \n",
    "Discover a comprehensive approach to constructing credit risk models. We employ various machine learning algorithms like LightGBM and CatBoost, alongside ensemble techniques for robust predictions. Our pipeline emphasizes data integrity, feature relevance, and model stability, crucial elements in credit risk assessment. \n",
    "## Features \n",
    "- **Data Preprocessing**: Begin with cleaning data, handling missing values, and optimizing memory usage for efficient computation.\n",
    "- **Feature Engineering**: Extract meaningful insights from data using advanced techniques, enhancing model predictive power.\n",
    "- **Model Training**: Train multiple machine learning models such as LightGBM and CatBoost to capture complex relationships and patterns.\n",
    "- **Ensemble Learning**: Combine predictions from various models using our custom Voting Model to achieve higher accuracy and stability. \n",
    "\n",
    "## Requirements\n",
    "\n",
    "- Libraries: NumPy, pandas, polars, seaborn, matplotlib, scikit-learn, lightgbm, imbalanced-learn, joblib, catboost\n",
    "\n",
    "## Usage \n",
    "Follow these steps:\n",
    "1. **Data Loading**: Ensure required datasets are available in the specified directory (`/kaggle/input/home-credit-credit-risk-model-stability`).\n",
    "2. **Initialization**: Run initialization code to set up necessary functions and configurations.\n",
    "3. **Data Preprocessing**: Execute data preprocessing steps to handle missing values and optimize memory usage.\n",
    "4. **Feature Engineering**: Use provided feature engineering functions to extract relevant features from the dataset.\n",
    "5. **Model Training**: Train machine learning models like LightGBM and CatBoost using preprocessed data.\n",
    "6. **Ensemble Learning**: Combine predictions from multiple models using the custom Voting Model for improved performance.\n",
    "7. **Evaluation**: Assess ensemble model performance and generate submission files for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a339c5f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:36.763099Z",
     "iopub.status.busy": "2024-05-26T09:49:36.762787Z",
     "iopub.status.idle": "2024-05-26T09:49:43.550365Z",
     "shell.execute_reply": "2024-05-26T09:49:43.549290Z"
    },
    "papermill": {
     "duration": 6.805393,
     "end_time": "2024-05-26T09:49:43.553051",
     "exception": false,
     "start_time": "2024-05-26T09:49:36.747658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys  # System-specific parameters and functions\n",
    "import subprocess  # Spawn new processes, connect to their input/output/error pipes, and obtain their return codes\n",
    "import os  # Operating system dependent functionality\n",
    "import gc  # Garbage Collector interface\n",
    "from pathlib import Path  # Object-oriented filesystem paths\n",
    "from glob import glob  # Unix style pathname pattern expansion\n",
    "\n",
    "import numpy as np  # Fundamental package for scientific computing with Python\n",
    "import pandas as pd  # Powerful data structures for data manipulation and analysis\n",
    "import polars as pl  # Fast DataFrame library implemented in Rust\n",
    "\n",
    "from datetime import datetime  # Basic date and time types\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import matplotlib.pyplot as plt  # MATLAB-like plotting framework\n",
    "\n",
    "import joblib  # Save and load Python objects\n",
    "\n",
    "import warnings  # Warning control\n",
    "warnings.filterwarnings('ignore')  # Ignore warnings\n",
    "\n",
    "from sklearn.base import BaseEstimator, RegressorMixin  # Base classes for all estimators in scikit-learn\n",
    "from sklearn.metrics import roc_auc_score  # ROC AUC score\n",
    "import lightgbm as lgb  # LightGBM: Gradient boosting framework\n",
    "from sklearn.model_selection import TimeSeriesSplit, GroupKFold, StratifiedGroupKFold  # Cross-validation strategies\n",
    "from imblearn.over_sampling import SMOTE  # Oversampling technique for imbalanced datasets\n",
    "from sklearn.preprocessing import OrdinalEncoder  # Encode categorical features as an integer array\n",
    "from sklearn.impute import KNNImputer  # Imputation for completing missing values using k-Nearest Neighbors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cae2c01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.585286Z",
     "iopub.status.busy": "2024-05-26T09:49:43.584671Z",
     "iopub.status.idle": "2024-05-26T09:49:43.588954Z",
     "shell.execute_reply": "2024-05-26T09:49:43.588084Z"
    },
    "papermill": {
     "duration": 0.02242,
     "end_time": "2024-05-26T09:49:43.591078",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.568658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT = '/kaggle/input/home-credit-credit-risk-model-stability'  # Setting the root directory path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167836a7",
   "metadata": {
    "papermill": {
     "duration": 0.015391,
     "end_time": "2024-05-26T09:49:43.620964",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.605573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preprocessing \n",
    "\n",
    "Let's create a  class named `Pipeline` containing methods to preprocess data using Pandas and Pipelines. \n",
    "**1. `set_table_dtypes(df)`**\n",
    "- This method iterates through each column in the DataFrame (`df`) and converts the data types based on certain conditions.\n",
    "- If the column name is one of [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"], it converts the column to `Int64`.\n",
    "- If the column name is \"date_decision\", it converts the column to `Date`.\n",
    "- If the last character of the column name is \"P\" or \"A\", it converts the column to `Float64`.\n",
    "- If the last character of the column name is \"M\", it converts the column to `String`.\n",
    "- If the last character of the column name is \"D\", it converts the column to `Date`.\n",
    "- Finally, it returns the DataFrame with modified data types.\n",
    "\n",
    "**2. `handle_dates(df)`**\n",
    "- This method aims to handle date columns in the DataFrame.\n",
    "- It iterates through each column, and if the last character of the column name is \"D\", it performs some operations.\n",
    "- It subtracts the date values in the current column from the values in the \"date_decision\" column.\n",
    "- Then it computes the total days between the two dates.\n",
    "- After processing, it drops the \"date_decision\" and \"MONTH\" columns from the DataFrame.\n",
    "- Finally, it returns the modified DataFrame.\n",
    "\n",
    "**3. `filter_cols(df)`**\n",
    "- This method filters out columns based on certain conditions.\n",
    "- It iterates through each column and checks if the column name is not in [\"target\", \"case_id\", \"WEEK_NUM\"] and if the column type is `String`.\n",
    "- If the number of unique values in the column is either 1 or more than 200, it drops that column.\n",
    "- Finally, it returns the filtered DataFrame.\n",
    "\n",
    "### Study Sources\n",
    "- For learning Pandas and data preprocessing: [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- Understanding Pipelines in data preprocessing: [Scikit-Learn Pipeline Documentation](https://scikit-learn.org/stable/modules/compose.html#pipeline)\n",
    "- Data type conversion and manipulation: [Pandas Data Types and Conversion](https://pandas.pydata.org/pandas-docs/version/1.3/user_guide/basics.html#basics-dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66c9b333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.651616Z",
     "iopub.status.busy": "2024-05-26T09:49:43.651271Z",
     "iopub.status.idle": "2024-05-26T09:49:43.662942Z",
     "shell.execute_reply": "2024-05-26T09:49:43.661874Z"
    },
    "papermill": {
     "duration": 0.029463,
     "end_time": "2024-05-26T09:49:43.664988",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.635525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 220): #200\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffff84",
   "metadata": {
    "papermill": {
     "duration": 0.014869,
     "end_time": "2024-05-26T09:49:43.694520",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.679651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Aggregator for Feature Extraction \n",
    "\n",
    "Let's create a  `Aggregator` class  to aggregate features from a DataFrame. \n",
    "\n",
    "**1. `num_expr(df)`**\n",
    "- This method extracts numerical features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"P\" or \"A\", indicating some numerical measurements.\n",
    "- For each selected column, it creates an expression to compute the maximum value and aliases it accordingly.\n",
    "- Finally, it returns a list of expressions for maximum values of numerical features.\n",
    "\n",
    " **2. `date_expr(df)`**\n",
    "- This method extracts date-related features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"D\", representing date columns.\n",
    "- Similar to `num_expr`, it creates expressions to compute the maximum date value for each selected column and aliases them.\n",
    "- It returns a list of expressions for maximum date values of date features.\n",
    "\n",
    " **3. `str_expr(df)`**\n",
    "- This method extracts string features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"M\", indicating string type columns.\n",
    "- It creates expressions to compute the maximum string value for each selected column and aliases them accordingly.\n",
    "- Returns a list of expressions for maximum string values of string features.\n",
    "\n",
    " **4. `other_expr(df)`**\n",
    "- This method extracts other miscellaneous features from the DataFrame (`df`).\n",
    "- It selects columns whose names end with \"T\" or \"L\".\n",
    "- Similar to previous methods, it computes the maximum value for each selected column and aliases them.\n",
    "- Returns a list of expressions for maximum values of miscellaneous features.\n",
    "\n",
    " **5. `count_expr(df)`**\n",
    "- This method extracts count-related features from the DataFrame (`df`).\n",
    "- It selects columns containing \"num_group\" in their names.\n",
    "- It computes the maximum value for each selected column and aliases them.\n",
    "- Returns a list of expressions for maximum count values of count features.\n",
    "\n",
    "**6. `get_exprs(df)`**\n",
    "- This method aggregates all the expressions from the previous methods to get a comprehensive list of feature extraction expressions.\n",
    "- It calls all the individual feature extraction methods and concatenates the resulting lists.\n",
    "- Returns a consolidated list of expressions for all types of features.\n",
    "\n",
    "### Study Sources\n",
    "- For learning about feature extraction and aggregation: [Feature Engineering for Machine Learning](https://www.amazon.com/Feature-Engineering-Machine-Learning-Principles/dp/1491953241)\n",
    "- Understanding Pandas DataFrame manipulation: [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
    "- Relational Algebra and Expressions: [Relational Algebra - Wikipedia](https://en.wikipedia.org/wiki/Relational_algebra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae570966",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.724603Z",
     "iopub.status.busy": "2024-05-26T09:49:43.724284Z",
     "iopub.status.idle": "2024-05-26T09:49:43.734837Z",
     "shell.execute_reply": "2024-05-26T09:49:43.733972Z"
    },
    "papermill": {
     "duration": 0.02791,
     "end_time": "2024-05-26T09:49:43.736848",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.708938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Aggregator:\n",
    "    #Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return expr_max\n",
    "    \n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max\n",
    "    \n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max\n",
    "    \n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        return  expr_max \n",
    "    \n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols] \n",
    "        return  expr_max\n",
    "    \n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b828004",
   "metadata": {
    "papermill": {
     "duration": 0.014615,
     "end_time": "2024-05-26T09:49:43.766290",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.751675",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# File Reading with Data Preprocessing \n",
    "\n",
    "The function `read_file(path, depth=None)` reads a Parquet file located at the given `path`, performs data preprocessing using the `Pipeline` class, and optionally aggregates features based on the `depth` parameter using the `Aggregator` class. \n",
    "\n",
    "1. `read_file(path, depth=None)`\n",
    "- **Inputs**:\n",
    "  - `path`: Path to the Parquet file.\n",
    "  - `depth`: An optional parameter indicating the depth of feature aggregation. Default is `None`.\n",
    "- **Output**: Returns a processed DataFrame.\n",
    "- **Process**:\n",
    "  - Reads the Parquet file located at the given `path` using `pl.read_parquet(path)`.\n",
    "  - Performs data preprocessing using the `Pipeline` class by applying the `set_table_dtypes` method to ensure proper data types.\n",
    "  - If `depth` is provided and is either 1 or 2:\n",
    "    - It groups the DataFrame by \"case_id\".\n",
    "    - It aggregates features based on the depth using the `Aggregator` class and the `get_exprs` method.\n",
    "  - Returns the processed DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b901143b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.795823Z",
     "iopub.status.busy": "2024-05-26T09:49:43.795562Z",
     "iopub.status.idle": "2024-05-26T09:49:43.800538Z",
     "shell.execute_reply": "2024-05-26T09:49:43.799627Z"
    },
    "papermill": {
     "duration": 0.021809,
     "end_time": "2024-05-26T09:49:43.802402",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.780593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c088dd",
   "metadata": {
    "papermill": {
     "duration": 0.014174,
     "end_time": "2024-05-26T09:49:43.830816",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.816642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading Multiple Files with Data Preprocessing \n",
    "\n",
    "Let's create a function `read_files(regex_path, depth=None)` that reads multiple Parquet files matching the specified regex pattern, performs data preprocessing using the `Pipeline` class, optionally aggregates features based on the `depth` parameter using the `Aggregator` class, and concatenates the results.\n",
    "**1. `read_files(regex_path, depth=None)`**\n",
    "- **Inputs**:\n",
    "  - `regex_path`: Regular expression pattern for matching file paths.\n",
    "  - `depth`: An optional parameter indicating the depth of feature aggregation. Default is `None`.\n",
    "- **Output**: Returns a concatenated and processed DataFrame.\n",
    "- **Process**:\n",
    "  - Initializes an empty list `chunks` to store processed DataFrames.\n",
    "  - Iterates through each file path matched by the provided regular expression pattern using `glob(str(regex_path))`.\n",
    "    - Reads each Parquet file using `pl.read_parquet(path)`.\n",
    "    - Performs data preprocessing using the `Pipeline` class by applying the `set_table_dtypes` method.\n",
    "    - If `depth` is provided and is either 1 or 2:\n",
    "      - It groups the DataFrame by \"case_id\".\n",
    "      - It aggregates features based on the depth using the `Aggregator` class and the `get_exprs` method.\n",
    "    - Appends the processed DataFrame to the `chunks` list.\n",
    "  - Concatenates all DataFrames in `chunks` vertically using `pl.concat(chunks, how=\"vertical_relaxed\")`.\n",
    "  - Removes duplicate rows based on the \"case_id\" column using `df.unique(subset=[\"case_id\"])`.\n",
    "  - Returns the concatenated and processed DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b7ba86",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.862492Z",
     "iopub.status.busy": "2024-05-26T09:49:43.862220Z",
     "iopub.status.idle": "2024-05-26T09:49:43.867888Z",
     "shell.execute_reply": "2024-05-26T09:49:43.867095Z"
    },
    "papermill": {
     "duration": 0.022922,
     "end_time": "2024-05-26T09:49:43.869809",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.846887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f4fda",
   "metadata": {
    "papermill": {
     "duration": 0.013839,
     "end_time": "2024-05-26T09:49:43.898132",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.884293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering Function \n",
    "\n",
    "Function `feature_eng(df_base, depth_0, depth_1, depth_2)` performs feature engineering on a base DataFrame (`df_base`) and multiple sets of additional DataFrames (`depth_0`, `depth_1`, `depth_2`). It adds new features, joins additional DataFrames, and handles dates using the `Pipeline` class.\n",
    "\n",
    " **1. `feature_eng(df_base, depth_0, depth_1, depth_2)`**\n",
    "- **Inputs**:\n",
    "  - `df_base`: Base DataFrame on which feature engineering will be performed.\n",
    "  - `depth_0`, `depth_1`, `depth_2`: Lists of DataFrames representing additional features of different depths.\n",
    "- **Output**: Returns the feature-engineered DataFrame.\n",
    "- **Process**:\n",
    "  - Adds new features to the base DataFrame:\n",
    "    - `month_decision`: Extracts the month from the \"date_decision\" column.\n",
    "    - `weekday_decision`: Extracts the weekday from the \"date_decision\" column.\n",
    "  - Iterates through each set of additional DataFrames (`depth_0`, `depth_1`, `depth_2`):\n",
    "    - Joins each DataFrame to the base DataFrame using the \"case_id\" column as the key and left join method.\n",
    "    - Appends a suffix to the column names to distinguish between different sets of features.\n",
    "  - Performs date handling using the `Pipeline` class by applying the `handle_dates` method.\n",
    "  - Returns the feature-engineered DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae2e607",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.928279Z",
     "iopub.status.busy": "2024-05-26T09:49:43.928018Z",
     "iopub.status.idle": "2024-05-26T09:49:43.933666Z",
     "shell.execute_reply": "2024-05-26T09:49:43.932880Z"
    },
    "papermill": {
     "duration": 0.02282,
     "end_time": "2024-05-26T09:49:43.935483",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.912663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ec915",
   "metadata": {
    "papermill": {
     "duration": 0.015075,
     "end_time": "2024-05-26T09:49:43.965396",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.950321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# DataFrame Conversion to Pandas with Categorical Columns \n",
    "\n",
    "The function `to_pandas(df_data, cat_cols=None)` converts a DataFrame (`df_data`) to a Pandas DataFrame and optionally converts specified columns to categorical data type. \n",
    "\n",
    " **1. `to_pandas(df_data, cat_cols=None)`**\n",
    "- **Inputs**:\n",
    "  - `df_data`: Input DataFrame to be converted to Pandas.\n",
    "  - `cat_cols`: Optional list of column names to be converted to categorical data type. Default is `None`.\n",
    "- **Output**: Returns the converted Pandas DataFrame and the list of categorical column names.\n",
    "- **Process**:\n",
    "  - Converts the input DataFrame to Pandas DataFrame using the `.to_pandas()` method.\n",
    "  - If `cat_cols` is not provided, it selects columns with data type \"object\" as default categorical columns.\n",
    "  - Converts the selected categorical columns to the categorical data type using `.astype(\"category\")`.\n",
    "  - Returns the converted Pandas DataFrame along with the list of categorical column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af92612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:43.995900Z",
     "iopub.status.busy": "2024-05-26T09:49:43.995635Z",
     "iopub.status.idle": "2024-05-26T09:49:44.000487Z",
     "shell.execute_reply": "2024-05-26T09:49:43.999427Z"
    },
    "papermill": {
     "duration": 0.021887,
     "end_time": "2024-05-26T09:49:44.002337",
     "exception": false,
     "start_time": "2024-05-26T09:49:43.980450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece7be7",
   "metadata": {
    "papermill": {
     "duration": 0.01493,
     "end_time": "2024-05-26T09:49:44.032499",
     "exception": false,
     "start_time": "2024-05-26T09:49:44.017569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Memory Reduction Function for DataFrames \n",
    "\n",
    "The function `reduce_mem_usage(df)` iterates through all columns of a DataFrame and modifies the data types to reduce memory usage. \n",
    "\n",
    " **1. `reduce_mem_usage(df)`**\n",
    "- **Input**: \n",
    "  - `df`: Input DataFrame.\n",
    "- **Output**: Returns the DataFrame with reduced memory usage.\n",
    "- **Process**:\n",
    "  - Calculates the initial memory usage of the DataFrame (`start_mem`) using `df.memory_usage()`.\n",
    "  - Iterates through each column of the DataFrame:\n",
    "    - Checks if the column type is a category. If so, skips to the next column.\n",
    "    - For non-category columns:\n",
    "      - Determines the minimum and maximum values of the column (`c_min` and `c_max`).\n",
    "      - If the column type is integer:\n",
    "        - Checks if the data can be fit into `int8`, `int16`, `int32`, or `int64` and converts the column type accordingly.\n",
    "      - If the column type is float:\n",
    "        - Checks if the data can be fit into `float16`, `float32`, or `float64` and converts the column type accordingly.\n",
    "      - If the column type is object (string), it skips the conversion.\n",
    "  - Calculates the final memory usage of the DataFrame (`end_mem`) after the modifications.\n",
    "- **Returns** the DataFrame with reduced memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd358d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:44.062976Z",
     "iopub.status.busy": "2024-05-26T09:49:44.062695Z",
     "iopub.status.idle": "2024-05-26T09:49:44.074712Z",
     "shell.execute_reply": "2024-05-26T09:49:44.074039Z"
    },
    "papermill": {
     "duration": 0.029513,
     "end_time": "2024-05-26T09:49:44.076673",
     "exception": false,
     "start_time": "2024-05-26T09:49:44.047160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997ff4b9",
   "metadata": {
    "papermill": {
     "duration": 0.01462,
     "end_time": "2024-05-26T09:49:44.106627",
     "exception": false,
     "start_time": "2024-05-26T09:49:44.092007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " **Definition of Root Directory and Subdirectories**\n",
    "- `ROOT`: It specifies the root directory path where the dataset is located. The `Path` object is created using the `Path` class from the `pathlib` module.\n",
    "- `TRAIN_DIR`: It specifies the directory path for training data files. It is derived from the `ROOT` directory by appending the subdirectories \"parquet_files\" and \"train\" using the `/` operator.\n",
    "- `TEST_DIR`: It specifies the directory path for test data files. Similar to `TRAIN_DIR`, it is derived from the `ROOT` directory by appending the subdirectories \"parquet_files\" and \"test\" using the `/` operator.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a2808f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:44.138287Z",
     "iopub.status.busy": "2024-05-26T09:49:44.137998Z",
     "iopub.status.idle": "2024-05-26T09:49:44.142570Z",
     "shell.execute_reply": "2024-05-26T09:49:44.141597Z"
    },
    "papermill": {
     "duration": 0.022878,
     "end_time": "2024-05-26T09:49:44.144680",
     "exception": false,
     "start_time": "2024-05-26T09:49:44.121802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TRAIN_DIR       = ROOT / \"parquet_files\" / \"train\"\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72413979",
   "metadata": {
    "papermill": {
     "duration": 0.015633,
     "end_time": "2024-05-26T09:49:44.176402",
     "exception": false,
     "start_time": "2024-05-26T09:49:44.160769",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination** \n",
    "\n",
    "\n",
    "Initializes a dictionary `data_store` containing different sets of DataFrames obtained from reading Parquet files using the `read_file()` and `read_files()` functions.\n",
    "\n",
    "****1. Data Store Initialization**\n",
    "- `data_store`: It is a dictionary storing different sets of DataFrames under different keys.\n",
    "  \n",
    "**2. Data Read Operations**\n",
    "- `df_base`: It stores the DataFrame obtained by reading the file \"train_base.parquet\" located in the `TRAIN_DIR` directory using the `read_file()` function.\n",
    "- `depth_0`: It stores a list of DataFrames obtained by reading multiple files. The first element is obtained using the `read_file()` function, while the second element is obtained using the `read_files()` function with a wildcard pattern.\n",
    "- `depth_1`: It stores a list of DataFrames obtained by reading multiple files using the `read_files()` function with specific patterns. Each file is associated with a depth level of 1.\n",
    "- `depth_2`: It stores a list containing a single DataFrame obtained by reading a specific file associated with a depth level of 2 using the `read_file()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ea13370",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:49:44.208468Z",
     "iopub.status.busy": "2024-05-26T09:49:44.208185Z",
     "iopub.status.idle": "2024-05-26T09:50:56.415518Z",
     "shell.execute_reply": "2024-05-26T09:50:56.414705Z"
    },
    "papermill": {
     "duration": 72.22593,
     "end_time": "2024-05-26T09:50:56.417880",
     "exception": false,
     "start_time": "2024-05-26T09:49:44.191950",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TRAIN_DIR / \"train_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TRAIN_DIR / \"train_static_cb_0.parquet\"),\n",
    "        read_files(TRAIN_DIR / \"train_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TRAIN_DIR / \"train_applprev_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TRAIN_DIR / \"train_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_other_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_person_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_deposit_1.parquet\", 1),\n",
    "        read_file(TRAIN_DIR / \"train_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TRAIN_DIR / \"train_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4679402",
   "metadata": {
    "papermill": {
     "duration": 0.014938,
     "end_time": "2024-05-26T09:50:56.448348",
     "exception": false,
     "start_time": "2024-05-26T09:50:56.433410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Data Preprocessing and Feature Engineering \n",
    "\n",
    "Perform data preprocessing and feature engineering operations on the training data.\n",
    "\n",
    "**1. Data Preprocessing and Feature Engineering**\n",
    "- `df_train = feature_eng(**data_store)`: Applies feature engineering to the training data stored in the `data_store` dictionary using the `feature_eng` function. The unpacking operator `**` is used to pass the dictionary as keyword arguments.\n",
    "- `del data_store`: Deletes the `data_store` dictionary to release memory.\n",
    "- `gc.collect()`: Manually triggers garbage collection to free up memory space.\n",
    "\n",
    " **2. Data Filtering, Conversion, and Memory Reduction**\n",
    "- `df_train = df_train.pipe(Pipeline.filter_cols)`: Applies column filtering using the `filter_cols` method from the `Pipeline` class to the `df_train` DataFrame using the `pipe` method.\n",
    "- `df_train, cat_cols = to_pandas(df_train)`: Converts the `df_train` DataFrame to Pandas DataFrame and retrieves the categorical column names. It uses the `to_pandas` function for conversion.\n",
    "- `df_train = reduce_mem_usage(df_train)`: Reduces memory usage of the `df_train` DataFrame using the `reduce_mem_usage` function to optimize memory consumption.\n",
    "\n",
    " **3. Handling Missing Values**\n",
    "- `nums = df_train.select_dtypes(exclude='category').columns`: Selects numerical columns (excluding categorical columns) from the DataFrame and stores their column names in the `nums` variable.\n",
    "- `from itertools import combinations, permutations`: Imports the `combinations` and `permutations` functions from the `itertools` module.\n",
    "- `nans_df = df_train[nums].isna()`: Creates a DataFrame `nans_df` to identify missing values in numerical columns.\n",
    "- `nans_groups = {}`: Initializes an empty dictionary to store numerical columns grouped by the count of missing values.\n",
    "- Loops through each numerical column (`col`) and calculates the count of missing values for each column. Then, it groups the columns based on the count of missing values in the `nans_groups` dictionary.\n",
    "\n",
    " **4. Memory Management**\n",
    "- `del nans_df`: Deletes the `nans_df` DataFrame to release memory.\n",
    "- `x = gc.collect()`: Manually triggers garbage collection to free up memory space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44667f38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:50:56.521739Z",
     "iopub.status.busy": "2024-05-26T09:50:56.521384Z",
     "iopub.status.idle": "2024-05-26T09:51:44.830228Z",
     "shell.execute_reply": "2024-05-26T09:51:44.829172Z"
    },
    "papermill": {
     "duration": 48.327687,
     "end_time": "2024-05-26T09:51:44.832712",
     "exception": false,
     "start_time": "2024-05-26T09:50:56.505025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = feature_eng(**data_store)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_train = df_train.pipe(Pipeline.filter_cols)\n",
    "df_train, cat_cols = to_pandas(df_train)\n",
    "df_train = reduce_mem_usage(df_train)\n",
    "nums=df_train.select_dtypes(exclude='category').columns\n",
    "from itertools import combinations, permutations\n",
    "nans_df = df_train[nums].isna()\n",
    "nans_groups={}\n",
    "for col in nums:\n",
    "    cur_group = nans_df[col].sum()\n",
    "    try:\n",
    "        nans_groups[cur_group].append(col)\n",
    "    except:\n",
    "        nans_groups[cur_group]=[col]\n",
    "del nans_df; x=gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f590367e",
   "metadata": {
    "papermill": {
     "duration": 0.015847,
     "end_time": "2024-05-26T09:51:44.865217",
     "exception": false,
     "start_time": "2024-05-26T09:51:44.849370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "Function `reduce_group(grps)` aims to reduce the number of columns within each group by selecting the column with the highest number of unique values. \n",
    "\n",
    "**1. `reduce_group(grps)`**\n",
    "- **Input**:\n",
    "  - `grps`: List of groups, where each group is represented as a list of column names.\n",
    "- **Output**: Returns a list of selected columns within each group.\n",
    "- **Process**:\n",
    "  - Initializes an empty list `use` to store the selected columns within each group.\n",
    "  - Iterates through each group `g` in the input list `grps`.\n",
    "    - Initializes variables `mx` and `vx` to track the maximum number of unique values and the corresponding column name within the group, respectively.\n",
    "    - Iterates through each column `gg` in the group `g`.\n",
    "      - Calculates the number of unique values `n` in the column `df_train[gg]`.\n",
    "      - Updates `mx` and `vx` if `n` is greater than the current maximum number of unique values.\n",
    "    - Appends the column name `vx` with the highest number of unique values to the `use` list for the current group.\n",
    "- **Returns** the list `use` containing selected columns within each group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5741f9d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:51:44.898269Z",
     "iopub.status.busy": "2024-05-26T09:51:44.897898Z",
     "iopub.status.idle": "2024-05-26T09:51:44.903951Z",
     "shell.execute_reply": "2024-05-26T09:51:44.902785Z"
    },
    "papermill": {
     "duration": 0.024972,
     "end_time": "2024-05-26T09:51:44.906040",
     "exception": false,
     "start_time": "2024-05-26T09:51:44.881068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_group(grps):\n",
    "    use = []\n",
    "    for g in grps:\n",
    "        mx = 0; vx = g[0]\n",
    "        for gg in g:\n",
    "            n = df_train[gg].nunique()\n",
    "            if n>mx:\n",
    "                mx = n\n",
    "                vx = gg\n",
    "        use.append(vx)\n",
    "    return use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbf98e2",
   "metadata": {
    "papermill": {
     "duration": 0.015071,
     "end_time": "2024-05-26T09:51:44.937310",
     "exception": false,
     "start_time": "2024-05-26T09:51:44.922239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination** \n",
    "\n",
    "Function, `group_columns_by_correlation(matrix, threshold=0.8)`, aims to group columns based on their correlation values. \n",
    "\n",
    " **1. `group_columns_by_correlation(matrix, threshold=0.8)`**\n",
    "- **Inputs**:\n",
    "  - `matrix`: DataFrame representing the dataset.\n",
    "  - `threshold`: Threshold value for correlation. Columns with correlation values greater than or equal to this threshold will be grouped together. Default is set to 0.8.\n",
    "- **Output**: Returns a list of column groups where each group contains columns with correlation values above the specified threshold.\n",
    "- **Process**:\n",
    "  - Calculates the correlation matrix of the input DataFrame `matrix` using the `.corr()` method.\n",
    "  - Initializes an empty list `groups` to store the resulting column groups.\n",
    "  - Initializes a list `remaining_cols` containing all column names from the DataFrame.\n",
    "  - Iterates through each column `col` in the `remaining_cols` list:\n",
    "    - Initializes a group with the current column `col`.\n",
    "    - Initializes a list `correlated_cols` containing the current column `col`.\n",
    "    - Iterates through each remaining column `c` in the `remaining_cols` list:\n",
    "      - If the correlation between the current column `col` and column `c` is greater than or equal to the specified `threshold`, adds column `c` to the group and `correlated_cols`.\n",
    "    - Appends the current group to the `groups` list.\n",
    "    - Updates the `remaining_cols` list to exclude columns already correlated with the current column.\n",
    "- **Returns** the list of column groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce009dac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:51:44.968688Z",
     "iopub.status.busy": "2024-05-26T09:51:44.968394Z",
     "iopub.status.idle": "2024-05-26T09:51:44.974827Z",
     "shell.execute_reply": "2024-05-26T09:51:44.973958Z"
    },
    "papermill": {
     "duration": 0.024569,
     "end_time": "2024-05-26T09:51:44.976768",
     "exception": false,
     "start_time": "2024-05-26T09:51:44.952199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def group_columns_by_correlation(matrix, threshold=0.75): #0.8\n",
    "    correlation_matrix = matrix.corr()\n",
    "    groups = []\n",
    "    remaining_cols = list(matrix.columns)\n",
    "    while remaining_cols:\n",
    "        col = remaining_cols.pop(0)\n",
    "        group = [col]\n",
    "        correlated_cols = [col]\n",
    "        for c in remaining_cols:\n",
    "            if correlation_matrix.loc[col, c] >= threshold:\n",
    "                group.append(c)\n",
    "                correlated_cols.append(c)\n",
    "        groups.append(group)\n",
    "        remaining_cols = [c for c in remaining_cols if c not in correlated_cols]\n",
    "    \n",
    "    return groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f3978f",
   "metadata": {
    "papermill": {
     "duration": 0.014614,
     "end_time": "2024-05-26T09:51:45.007037",
     "exception": false,
     "start_time": "2024-05-26T09:51:44.992423",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Handling Missing Values and Reducing Columns Based on Correlation\n",
    "\n",
    "Let's processes the `nans_groups` dictionary to handle missing values and reduce columns based on their correlation. \n",
    "### Explanation of Code\n",
    "\n",
    "1. **Initialization**\n",
    "   - `uses = []`: Initializes an empty list `uses` to store the final list of selected columns.\n",
    "\n",
    "2. **Iterate through Groups in `nans_groups`**\n",
    "   - `for k, v in nans_groups.items()`: Iterates through the `nans_groups` dictionary where `k` is the key (number of missing values) and `v` is the list of column names with that number of missing values.\n",
    "\n",
    "3. **Processing Each Group**\n",
    "   - **For Groups with More Than One Column**:\n",
    "     - `if len(v) > 1`: Checks if the group contains more than one column.\n",
    "       - `Vs = nans_groups[k]`: Assigns the list of columns `v` to `Vs`.\n",
    "       - `grps = group_columns_by_correlation(df_train[Vs], threshold=0.8)`: Groups columns in `Vs` based on their correlation using a threshold of 0.8.\n",
    "       - `use = reduce_group(grps)`: Reduces the groups by selecting columns with the highest number of unique values using the `reduce_group` function.\n",
    "       - `uses = uses + use`: Appends the selected columns to the `uses` list.\n",
    "   - **For Groups with a Single Column**:\n",
    "     - `else`: If the group contains only one column,\n",
    "       - `uses = uses + v`: Directly appends the column to the `uses` list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e0c79a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:51:45.037737Z",
     "iopub.status.busy": "2024-05-26T09:51:45.037464Z",
     "iopub.status.idle": "2024-05-26T09:52:07.163355Z",
     "shell.execute_reply": "2024-05-26T09:52:07.162470Z"
    },
    "papermill": {
     "duration": 22.14409,
     "end_time": "2024-05-26T09:52:07.166001",
     "exception": false,
     "start_time": "2024-05-26T09:51:45.021911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "uses=[]\n",
    "for k,v in nans_groups.items():\n",
    "    if len(v)>1:\n",
    "            Vs = nans_groups[k]\n",
    "            grps= group_columns_by_correlation(df_train[Vs], threshold=0.75)#0.8\n",
    "            use=reduce_group(grps)\n",
    "            uses=uses+use\n",
    "    else:\n",
    "        uses=uses+v\n",
    "\n",
    "# Subset the DataFrame to keep only the selected columns\n",
    "df_train = df_train[uses]        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1bc5e",
   "metadata": {
    "papermill": {
     "duration": 0.017761,
     "end_time": "2024-05-26T09:52:07.201611",
     "exception": false,
     "start_time": "2024-05-26T09:52:07.183850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation for Test Set\n",
    "\n",
    "Let's prepares the `data_store` dictionary for the test set by reading the required Parquet files. The structure and logic mirror those used for the training set, ensuring consistency in data preprocessing.\n",
    "### Explanation of Code\n",
    "\n",
    "1. **Reading Base and Depth Data for Test Set**:\n",
    "    - **Base Data**:\n",
    "      - `df_base`: Reads the base data from the Parquet file located at `TEST_DIR / \"test_base.parquet\"`.\n",
    "    - **Depth 0 Data**:\n",
    "      - `depth_0`: Reads the static credit bureau data from individual and wildcard Parquet files located at `TEST_DIR / \"test_static_cb_0.parquet\"` and `TEST_DIR / \"test_static_0_*.parquet\"`.\n",
    "    - **Depth 1 Data**:\n",
    "      - `depth_1`: Reads various related data files such as application previous, tax registry, credit bureau, and other related data files, all with depth 1, from their respective Parquet files.\n",
    "    - **Depth 2 Data**:\n",
    "      - `depth_2`: Reads the credit bureau data with depth 2 from the Parquet file located at `TEST_DIR / \"test_credit_bureau_b_2.parquet\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8a2cec9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:07.238187Z",
     "iopub.status.busy": "2024-05-26T09:52:07.237416Z",
     "iopub.status.idle": "2024-05-26T09:52:07.479259Z",
     "shell.execute_reply": "2024-05-26T09:52:07.478230Z"
    },
    "papermill": {
     "duration": 0.262972,
     "end_time": "2024-05-26T09:52:07.481719",
     "exception": false,
     "start_time": "2024-05-26T09:52:07.218747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "    ]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba71ae1",
   "metadata": {
    "papermill": {
     "duration": 0.015132,
     "end_time": "2024-05-26T09:52:07.512900",
     "exception": false,
     "start_time": "2024-05-26T09:52:07.497768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Engineering and Data Preparation for the Test Set ðŸš€\n",
    "\n",
    "Performs feature engineering and data preparation on the test set. It follows the same steps as for the training set, ensuring consistency in data processing. \n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **Feature Engineering**:\n",
    "    - `df_test = feature_eng(**data_store)`: Applies the `feature_eng` function to the test data stored in `data_store`. This function performs various feature engineering steps, such as creating new features and joining different depth data based on `case_id`.\n",
    "\n",
    "2. **Memory Management**:\n",
    "    - `del data_store`: Deletes the `data_store` dictionary to free up memory.\n",
    "    - `gc.collect()`: Calls the garbage collector to release any unreferenced memory.\n",
    "\n",
    "3. **Selecting Relevant Columns**:\n",
    "    - `df_test = df_test.select([col for col in df_train.columns if col != \"target\"])`: Selects columns in `df_test` that match the columns in `df_train`, excluding the \"target\" column. This ensures that the test set has the same features as the training set.\n",
    "\n",
    "4. **Conversion to Pandas DataFrame and Category Data Type**:\n",
    "    - `df_test, cat_cols = to_pandas(df_test)`: Converts the `df_test` Polars DataFrame to a Pandas DataFrame and converts specified columns to the \"category\" data type to save memory.\n",
    "\n",
    "5. **Memory Usage Reduction**:\n",
    "    - `df_test = reduce_mem_usage(df_test)`: Applies the `reduce_mem_usage` function to reduce the memory footprint of the Pandas DataFrame by converting columns to more efficient data types.\n",
    "\n",
    "6. **Final Memory Management**:\n",
    "    - `gc.collect()`: Calls the garbage collector again to release any unreferenced memory after data processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8970fd3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:07.545395Z",
     "iopub.status.busy": "2024-05-26T09:52:07.545070Z",
     "iopub.status.idle": "2024-05-26T09:52:07.923152Z",
     "shell.execute_reply": "2024-05-26T09:52:07.922344Z"
    },
    "papermill": {
     "duration": 0.397043,
     "end_time": "2024-05-26T09:52:07.925202",
     "exception": false,
     "start_time": "2024-05-26T09:52:07.528159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = feature_eng(**data_store)\n",
    "del data_store\n",
    "gc.collect()\n",
    "df_test = df_test.select([col for col in df_train.columns if col != \"target\"])\n",
    "df_test, cat_cols = to_pandas(df_test)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ab4610",
   "metadata": {
    "papermill": {
     "duration": 0.015237,
     "end_time": "2024-05-26T09:52:07.955836",
     "exception": false,
     "start_time": "2024-05-26T09:52:07.940599",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **Adding Target Column to Training Set**:\n",
    "    - `df_train['target'] = 0`: Adds a column named \"target\" to the `df_train` DataFrame and sets its value to 0 for all rows. This indicates that these rows belong to the training set.\n",
    "\n",
    "2. **Adding Target Column to Test Set**:\n",
    "    - `df_test['target'] = 1`: Adds a column named \"target\" to the `df_test` DataFrame and sets its value to 1 for all rows. This indicates that these rows belong to the test set.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3542db03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:07.989692Z",
     "iopub.status.busy": "2024-05-26T09:52:07.988815Z",
     "iopub.status.idle": "2024-05-26T09:52:07.996238Z",
     "shell.execute_reply": "2024-05-26T09:52:07.995332Z"
    },
    "papermill": {
     "duration": 0.026705,
     "end_time": "2024-05-26T09:52:07.998268",
     "exception": false,
     "start_time": "2024-05-26T09:52:07.971563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train['target']=0\n",
    "df_test['target']=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483499d",
   "metadata": {
    "papermill": {
     "duration": 0.015677,
     "end_time": "2024-05-26T09:52:08.030042",
     "exception": false,
     "start_time": "2024-05-26T09:52:08.014365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Combining and Preparing Data for Modeling ðŸš€ðŸ“Š\n",
    "\n",
    "Let's combines the training and test datasets, optimizes memory usage, prepares the features and target for modeling, and then saves the prepared data to a file using `joblib`. \n",
    "### Explanation of Code\n",
    "\n",
    "1. **Combining Train and Test Data**:\n",
    "    - `df_train = pd.concat([df_train, df_test])`: Concatenates the training and test datasets along the rows. This step combines the datasets into one for uniform preprocessing.\n",
    "\n",
    "2. **Reducing Memory Usage**:\n",
    "    - `df_train = reduce_mem_usage(df_train)`: Applies the `reduce_mem_usage` function to the combined DataFrame to optimize its memory usage by converting columns to more efficient data types.\n",
    "\n",
    "3. **Preparing Target Variable**:\n",
    "    - `y = df_train[\"target\"]`: Extracts the \"target\" column from the combined DataFrame and stores it in the variable `y`. This will be used as the target variable for modeling.\n",
    "\n",
    "4. **Dropping Unnecessary Columns**:\n",
    "    - `df_train = df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])`: Drops the \"target\", \"case_id\", and \"WEEK_NUM\" columns from the combined DataFrame. The \"case_id\" and \"WEEK_NUM\" columns are likely identifiers and not useful for modeling.\n",
    "\n",
    "5. **Saving the Prepared Data**:\n",
    "    - `joblib.dump((df_train, y, df_test), 'data.pkl')`: Uses `joblib` to save the prepared features (`df_train`), target (`y`), and test set (`df_test`) to a file named `data.pkl`. This serialized file can be loaded later for model training and evaluation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0bc590b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:08.061681Z",
     "iopub.status.busy": "2024-05-26T09:52:08.061401Z",
     "iopub.status.idle": "2024-05-26T09:52:20.046261Z",
     "shell.execute_reply": "2024-05-26T09:52:20.045393Z"
    },
    "papermill": {
     "duration": 12.003047,
     "end_time": "2024-05-26T09:52:20.048429",
     "exception": false,
     "start_time": "2024-05-26T09:52:08.045382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.concat([df_train,df_test])\n",
    "df_train=reduce_mem_usage(df_train)\n",
    "\n",
    "y = df_train[\"target\"]\n",
    "df_train= df_train.drop(columns=[\"target\", \"case_id\", \"WEEK_NUM\"])\n",
    "\n",
    "\n",
    "joblib.dump((df_train,y,df_test),'data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b2ea9",
   "metadata": {
    "papermill": {
     "duration": 0.015127,
     "end_time": "2024-05-26T09:52:20.079275",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.064148",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acff495f",
   "metadata": {
    "papermill": {
     "duration": 0.015203,
     "end_time": "2024-05-26T09:52:20.109843",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.094640",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Data Preprocessing with Pipeline Class ðŸš€\n",
    "\n",
    "Define a `Pipeline` class with methods to preprocess a DataFrame. The class includes methods for setting data types, handling date columns, and filtering columns based on certain criteria.\n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **set_table_dtypes(df)**:\n",
    "    - This method sets the appropriate data types for the columns in the DataFrame.\n",
    "    - **Int64**: Converts specified columns to 64-bit integers.\n",
    "    - **Date**: Converts specified columns to date type.\n",
    "    - **Float64**: Converts specified columns to 64-bit floating-point numbers.\n",
    "    - **String**: Converts specified columns to string type.\n",
    "  \n",
    "2. **handle_dates(df)**:\n",
    "    - This method handles date columns by calculating the difference in days between date columns ending with \"D\" and a reference date column \"date_decision\".\n",
    "    - It drops the \"date_decision\" and \"MONTH\" columns after the calculations.\n",
    "\n",
    "3. **filter_cols(df)**:\n",
    "    - This method filters out columns based on missing values and unique values.\n",
    "    - Columns with more than 70% missing values are dropped.\n",
    "    - String columns with either only one unique value or more than 200 unique values are dropped as they are likely not useful for modeling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42bfbd16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:20.142076Z",
     "iopub.status.busy": "2024-05-26T09:52:20.141740Z",
     "iopub.status.idle": "2024-05-26T09:52:20.153497Z",
     "shell.execute_reply": "2024-05-26T09:52:20.152727Z"
    },
    "papermill": {
     "duration": 0.029971,
     "end_time": "2024-05-26T09:52:20.155275",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.125304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Pipeline:\n",
    "\n",
    "    def set_table_dtypes(df):\n",
    "        for col in df.columns:\n",
    "            if col in [\"case_id\", \"WEEK_NUM\", \"num_group1\", \"num_group2\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Int64))\n",
    "            elif col in [\"date_decision\"]:\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "            elif col[-1] in (\"P\", \"A\"):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Float64))\n",
    "            elif col[-1] in (\"M\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.String))\n",
    "            elif col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col).cast(pl.Date))\n",
    "        return df\n",
    "\n",
    "    def handle_dates(df):\n",
    "        for col in df.columns:\n",
    "            if col[-1] in (\"D\",):\n",
    "                df = df.with_columns(pl.col(col) - pl.col(\"date_decision\"))  #!!?\n",
    "                df = df.with_columns(pl.col(col).dt.total_days()) # t - t-1\n",
    "        df = df.drop(\"date_decision\", \"MONTH\")\n",
    "        return df\n",
    "\n",
    "    def filter_cols(df):\n",
    "        for col in df.columns:\n",
    "            if col not in [\"target\", \"case_id\", \"WEEK_NUM\"]:\n",
    "                isnull = df[col].is_null().mean()\n",
    "                if isnull > 0.7:\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if (col not in [\"target\", \"case_id\", \"WEEK_NUM\"]) & (df[col].dtype == pl.String):\n",
    "                freq = df[col].n_unique()\n",
    "                if (freq == 1) | (freq > 220):#200\n",
    "                    df = df.drop(col)\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abdf38e",
   "metadata": {
    "papermill": {
     "duration": 0.016277,
     "end_time": "2024-05-26T09:52:20.186729",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.170452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Aggregation with Aggregator Class \n",
    "\n",
    "Define an `Aggregator` class designed to aggregate features from a DataFrame. Aggregation functions are used to transform and summarize data, which can help in creating new features for machine learning models. \n",
    "### Explanation of Code\n",
    "\n",
    "1. **num_expr(df)**:\n",
    "    - This method aggregates numerical columns ending with \"P\" or \"A\".\n",
    "    - **Max**: Maximum value of each column.\n",
    "    - **Last**: Last value of each column.\n",
    "    - **Mean**: Average value of each column.\n",
    "    - **Median** and **Variance** are also defined but not used in this implementation.\n",
    "\n",
    "2. **date_expr(df)**:\n",
    "    - This method aggregates date columns ending with \"D\".\n",
    "    - **Max**: Latest date in each column.\n",
    "    - **Last**: Last date in each column.\n",
    "    - **Mean**: Average date in each column.\n",
    "\n",
    "3. **str_expr(df)**:\n",
    "    - This method aggregates string columns ending with \"M\".\n",
    "    - **Max**: Lexicographically last string in each column.\n",
    "    - **Last**: Last string value in each column.\n",
    "\n",
    "4. **other_expr(df)**:\n",
    "    - This method aggregates other columns ending with \"T\" or \"L\".\n",
    "    - **Max**: Maximum value in each column.\n",
    "    - **Last**: Last value in each column.\n",
    "\n",
    "5. **count_expr(df)**:\n",
    "    - This method aggregates columns containing \"num_group\".\n",
    "    - **Max**: Maximum value in each column.\n",
    "    - **Last**: Last value in each column.\n",
    "\n",
    "6. **get_exprs(df)**:\n",
    "    - This method combines all the aggregation expressions from the above methods.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44c493a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:20.219986Z",
     "iopub.status.busy": "2024-05-26T09:52:20.219681Z",
     "iopub.status.idle": "2024-05-26T09:52:20.234772Z",
     "shell.execute_reply": "2024-05-26T09:52:20.233956Z"
    },
    "papermill": {
     "duration": 0.034163,
     "end_time": "2024-05-26T09:52:20.236613",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.202450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Aggregator:\n",
    "    # Please add or subtract features yourself, be aware that too many features will take up too much space.\n",
    "    def num_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"P\", \"A\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "        expr_var = [pl.var(col).alias(f\"var_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    def date_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"D\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        expr_mean = [pl.mean(col).alias(f\"mean_{col}\") for col in cols]\n",
    "        expr_median = [pl.median(col).alias(f\"median_{col}\") for col in cols]\n",
    "\n",
    "        return expr_max + expr_last + expr_mean \n",
    "\n",
    "    def str_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"M\",)]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        # expr_count = [pl.count(col).alias(f\"count_{col}\") for col in cols]\n",
    "        return expr_max + expr_last  # +expr_count\n",
    "\n",
    "    def other_expr(df):\n",
    "        cols = [col for col in df.columns if col[-1] in (\"T\", \"L\")]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def count_expr(df):\n",
    "        cols = [col for col in df.columns if \"num_group\" in col]\n",
    "        expr_max = [pl.max(col).alias(f\"max_{col}\") for col in cols]\n",
    "        # expr_min = [pl.min(col).alias(f\"min_{col}\") for col in cols]\n",
    "        expr_last = [pl.last(col).alias(f\"last_{col}\") for col in cols]\n",
    "        # expr_first = [pl.first(col).alias(f\"first_{col}\") for col in cols]\n",
    "        return expr_max + expr_last\n",
    "\n",
    "    def get_exprs(df):\n",
    "        exprs = Aggregator.num_expr(df) + \\\n",
    "                Aggregator.date_expr(df) + \\\n",
    "                Aggregator.str_expr(df) + \\\n",
    "                Aggregator.other_expr(df) + \\\n",
    "                Aggregator.count_expr(df)\n",
    "\n",
    "        return exprs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61a9d5",
   "metadata": {
    "papermill": {
     "duration": 0.015762,
     "end_time": "2024-05-26T09:52:20.269249",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.253487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Preparation Functions \n",
    "\n",
    "Functions to prepare and process data for our  machine learning pipeline. These functions include reading data from files, performing feature engineering, and converting data formats.\n",
    "\n",
    "### Explanation of Code\n",
    "\n",
    "1. **read_file(path, depth=None)**:\n",
    "    - **Purpose**: Reads a Parquet file, sets appropriate data types, and optionally performs aggregation based on `depth`.\n",
    "    - **Parameters**:\n",
    "        - `path`: File path to the Parquet file.\n",
    "        - `depth`: Determines the level of aggregation (1 or 2).\n",
    "    - **Process**:\n",
    "        - Reads the Parquet file into a DataFrame.\n",
    "        - Sets the appropriate data types using a pipeline.\n",
    "        - If `depth` is 1 or 2, groups the data by `case_id` and aggregates features using the `Aggregator` class.\n",
    "\n",
    "2. **read_files(regex_path, depth=None)**:\n",
    "    - **Purpose**: Reads multiple Parquet files matching a regex pattern, sets appropriate data types, and optionally performs aggregation.\n",
    "    - **Parameters**:\n",
    "        - `regex_path`: Regex pattern to match file paths.\n",
    "        - `depth`: Determines the level of aggregation (1 or 2).\n",
    "    - **Process**:\n",
    "        - Reads each Parquet file matching the regex pattern into a DataFrame.\n",
    "        - Sets the appropriate data types using a pipeline.\n",
    "        - If `depth` is 1 or 2, groups the data by `case_id` and aggregates features.\n",
    "        - Concatenates the DataFrames vertically and removes duplicate `case_id`s.\n",
    "\n",
    "3. **feature_eng(df_base, depth_0, depth_1, depth_2)**:\n",
    "    - **Purpose**: Performs feature engineering by adding date-related features and joining additional data based on `case_id`.\n",
    "    - **Parameters**:\n",
    "        - `df_base`: Base DataFrame.\n",
    "        - `depth_0`, `depth_1`, `depth_2`: Lists of DataFrames at different depths.\n",
    "    - **Process**:\n",
    "        - Adds `month_decision` and `weekday_decision` features based on `date_decision`.\n",
    "        - Joins additional DataFrames from `depth_0`, `depth_1`, and `depth_2` to the base DataFrame.\n",
    "        - Handles date columns using a pipeline.\n",
    "\n",
    "4. **to_pandas(df_data, cat_cols=None)**:\n",
    "    - **Purpose**: Converts a Polars DataFrame to a Pandas DataFrame and sets categorical data types.\n",
    "    - **Parameters**:\n",
    "        - `df_data`: Polars DataFrame to be converted.\n",
    "        - `cat_cols`: List of columns to be converted to categorical type.\n",
    "    - **Process**:\n",
    "        - Converts the Polars DataFrame to a Pandas DataFrame.\n",
    "        - If `cat_cols` is not provided, identifies columns of object type and converts them to categorical.\n",
    "        - Returns the converted DataFrame and the list of categorical columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26a0ff95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:20.301579Z",
     "iopub.status.busy": "2024-05-26T09:52:20.301316Z",
     "iopub.status.idle": "2024-05-26T09:52:20.312068Z",
     "shell.execute_reply": "2024-05-26T09:52:20.311331Z"
    },
    "papermill": {
     "duration": 0.029063,
     "end_time": "2024-05-26T09:52:20.313934",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.284871",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(path, depth=None):\n",
    "    df = pl.read_parquet(path)\n",
    "    df = df.pipe(Pipeline.set_table_dtypes)\n",
    "    if depth in [1,2]:\n",
    "        df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df)) \n",
    "    return df\n",
    "\n",
    "def read_files(regex_path, depth=None):\n",
    "    chunks = []\n",
    "    \n",
    "    for path in glob(str(regex_path)):\n",
    "        df = pl.read_parquet(path)\n",
    "        df = df.pipe(Pipeline.set_table_dtypes)\n",
    "        if depth in [1, 2]:\n",
    "            df = df.group_by(\"case_id\").agg(Aggregator.get_exprs(df))\n",
    "        chunks.append(df)\n",
    "    \n",
    "    df = pl.concat(chunks, how=\"vertical_relaxed\")\n",
    "    df = df.unique(subset=[\"case_id\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "def feature_eng(df_base, depth_0, depth_1, depth_2):\n",
    "    df_base = (\n",
    "        df_base\n",
    "        .with_columns(\n",
    "            month_decision = pl.col(\"date_decision\").dt.month(),\n",
    "            weekday_decision = pl.col(\"date_decision\").dt.weekday(),\n",
    "        )\n",
    "    )\n",
    "    for i, df in enumerate(depth_0 + depth_1 + depth_2):\n",
    "        df_base = df_base.join(df, how=\"left\", on=\"case_id\", suffix=f\"_{i}\")\n",
    "    df_base = df_base.pipe(Pipeline.handle_dates)\n",
    "    return df_base\n",
    "\n",
    "def to_pandas(df_data, cat_cols=None):\n",
    "    df_data = df_data.to_pandas()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = list(df_data.select_dtypes(\"object\").columns)\n",
    "    df_data[cat_cols] = df_data[cat_cols].astype(\"category\")\n",
    "    return df_data, cat_cols\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93de3a6",
   "metadata": {
    "papermill": {
     "duration": 0.015862,
     "end_time": "2024-05-26T09:52:20.345354",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.329492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Memory Optimization Function\n",
    "\n",
    "Function `reduce_mem_usage(df)` which takes a DataFrame `df` as input and iterates through all its columns. The purpose of this function is to optimize the memory usage of the DataFrame by adjusting the data types of its columns.\n",
    "**Explanation:**\n",
    "\n",
    "\n",
    "- The function starts by calculating the initial memory usage of the DataFrame `df`.\n",
    "- It then iterates through each column of the DataFrame.\n",
    "- For each column, it checks the data type.\n",
    "- If the column is categorical, it skips the optimization process.\n",
    "- For non-categorical columns, it finds the minimum and maximum values.\n",
    "- Based on the range of values, it changes the data type to one that requires less memory while ensuring that it can still accommodate the data without loss of precision.\n",
    "- Finally, it calculates the memory usage after optimization and prints out the reduction percentage.\n",
    "- The function returns the optimized DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2a86bb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:20.379600Z",
     "iopub.status.busy": "2024-05-26T09:52:20.379321Z",
     "iopub.status.idle": "2024-05-26T09:52:20.392380Z",
     "shell.execute_reply": "2024-05-26T09:52:20.391401Z"
    },
    "papermill": {
     "duration": 0.032549,
     "end_time": "2024-05-26T09:52:20.394402",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.361853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if str(col_type)==\"category\":\n",
    "            continue\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            continue\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6fff71",
   "metadata": {
    "papermill": {
     "duration": 0.016366,
     "end_time": "2024-05-26T09:52:20.427546",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.411180",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Information Retrieval \n",
    "\n",
    "**Explanation:**\n",
    "- The code loads information about a specific LightGBM model from a file named 'notebook_info.joblib'.\n",
    "- It prints out the start time of the notebook that created the models and a brief description of the notebook.\n",
    "- The code then retrieves details about the columns and categorical columns used in the models.\n",
    "- It prints out the number of columns and categorical columns.\n",
    "- Next, it loads the LightGBM models from a file named 'lgb_models.joblib'.\n",
    "- Finally, it displays the loaded LightGBM models.\n",
    "\n",
    "- Similarly, this part of the code loads information about categorical (cat) models and prints out the start time of the notebook that created the models and a brief description of the notebook.\n",
    "- It then loads the categorical (cat) models from a file named 'cat_models.joblib'.\n",
    "- Finally, it displays the loaded categorical models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b805363",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:20.461973Z",
     "iopub.status.busy": "2024-05-26T09:52:20.461640Z",
     "iopub.status.idle": "2024-05-26T09:52:25.772482Z",
     "shell.execute_reply": "2024-05-26T09:52:25.771443Z"
    },
    "papermill": {
     "duration": 5.330825,
     "end_time": "2024-05-26T09:52:25.774562",
     "exception": false,
     "start_time": "2024-05-26T09:52:20.443737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- [lgb] notebook_start_time: 2024-04-17 17:19:35.710340\n",
      "- [lgb] description: Add notebook info dict to store cols and cat_cols\n",
      "- [lgb] len(cols): 386\n",
      "- [lgb] len(cat_cols): 113\n",
      "- [cat] notebook_start_time: 2024-04-18 00:37:32.864485\n",
      "- [cat] description: first cat models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<catboost.core.CatBoostClassifier at 0x79a322da3fd0>,\n",
       " <catboost.core.CatBoostClassifier at 0x79a3b8a45810>,\n",
       " <catboost.core.CatBoostClassifier at 0x79a3b8a3f8e0>,\n",
       " <catboost.core.CatBoostClassifier at 0x79a2d6128430>,\n",
       " <catboost.core.CatBoostClassifier at 0x799e5d780d00>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/notebook_info.joblib')\n",
    "print(f\"- [lgb] notebook_start_time: {lgb_notebook_info['notebook_start_time']}\")\n",
    "print(f\"- [lgb] description: {lgb_notebook_info['description']}\")\n",
    "\n",
    "cols = lgb_notebook_info['cols']\n",
    "cat_cols = lgb_notebook_info['cat_cols']\n",
    "print(f\"- [lgb] len(cols): {len(cols)}\")\n",
    "print(f\"- [lgb] len(cat_cols): {len(cat_cols)}\")\n",
    "\n",
    "lgb_models = joblib.load('/kaggle/input/homecredit-models-public/other/lgb/1/lgb_models.joblib')\n",
    "lgb_models\n",
    "\n",
    "cat_notebook_info = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/notebook_info.joblib')\n",
    "print(f\"- [cat] notebook_start_time: {cat_notebook_info['notebook_start_time']}\")\n",
    "print(f\"- [cat] description: {cat_notebook_info['description']}\")\n",
    "\n",
    "cat_models = joblib.load('/kaggle/input/homecredit-models-public/other/cat/1/cat_models.joblib')\n",
    "cat_models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1358356",
   "metadata": {
    "papermill": {
     "duration": 0.016226,
     "end_time": "2024-05-26T09:52:25.807829",
     "exception": false,
     "start_time": "2024-05-26T09:52:25.791603",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading and Storage Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ede7e83b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:25.873739Z",
     "iopub.status.busy": "2024-05-26T09:52:25.873402Z",
     "iopub.status.idle": "2024-05-26T09:52:26.092911Z",
     "shell.execute_reply": "2024-05-26T09:52:26.092034Z"
    },
    "papermill": {
     "duration": 0.238953,
     "end_time": "2024-05-26T09:52:26.095124",
     "exception": false,
     "start_time": "2024-05-26T09:52:25.856171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ROOT            = Path(\"/kaggle/input/home-credit-credit-risk-model-stability\")\n",
    "\n",
    "TEST_DIR        = ROOT / \"parquet_files\" / \"test\"\n",
    "\n",
    "data_store = {\n",
    "    \"df_base\": read_file(TEST_DIR / \"test_base.parquet\"),\n",
    "    \"depth_0\": [\n",
    "        read_file(TEST_DIR / \"test_static_cb_0.parquet\"),\n",
    "        read_files(TEST_DIR / \"test_static_0_*.parquet\"),\n",
    "    ],\n",
    "    \"depth_1\": [\n",
    "        read_files(TEST_DIR / \"test_applprev_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_a_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_tax_registry_c_1.parquet\", 1),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_1_*.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_other_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_person_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_deposit_1.parquet\", 1),\n",
    "        read_file(TEST_DIR / \"test_debitcard_1.parquet\", 1),\n",
    "    ],\n",
    "    \"depth_2\": [\n",
    "        read_file(TEST_DIR / \"test_credit_bureau_b_2.parquet\", 2),\n",
    "        read_files(TEST_DIR / \"test_credit_bureau_a_2_*.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_applprev_2.parquet\", 2),\n",
    "        read_file(TEST_DIR / \"test_person_2.parquet\", 2)\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd1510c",
   "metadata": {
    "papermill": {
     "duration": 0.017082,
     "end_time": "2024-05-26T09:52:26.130050",
     "exception": false,
     "start_time": "2024-05-26T09:52:26.112968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Performs feature engineering on the test data using the `feature_eng` function with the provided data store.\n",
    "- After feature engineering, it prints out the shape of the processed test data.\n",
    "- Then, it deletes the data store and performs garbage collection to free up memory.\n",
    "- It selects only the required columns from the processed test data.\n",
    "- The test data is converted to a pandas DataFrame and categorical columns are handled accordingly.\n",
    "- Memory usage of the test data is optimized using the `reduce_mem_usage` function.\n",
    "- Finally, the 'case_id' column is set as the index for the test data, and its shape is printed out again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dab24d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:26.165595Z",
     "iopub.status.busy": "2024-05-26T09:52:26.165300Z",
     "iopub.status.idle": "2024-05-26T09:52:26.683756Z",
     "shell.execute_reply": "2024-05-26T09:52:26.682667Z"
    },
    "papermill": {
     "duration": 0.539289,
     "end_time": "2024-05-26T09:52:26.686507",
     "exception": false,
     "start_time": "2024-05-26T09:52:26.147218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data shape:\t (10, 860)\n",
      "Memory usage of dataframe is 0.04 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 40.2%\n",
      "test data shape:\t (10, 386)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = feature_eng(**data_store)\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "del data_store\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "df_test = df_test.select(['case_id'] + cols)\n",
    "\n",
    "df_test, cat_cols = to_pandas(df_test, cat_cols)\n",
    "df_test = reduce_mem_usage(df_test)\n",
    "df_test = df_test.set_index('case_id')\n",
    "print(\"test data shape:\\t\", df_test.shape)\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60dc59",
   "metadata": {
    "papermill": {
     "duration": 0.016789,
     "end_time": "2024-05-26T09:52:26.720971",
     "exception": false,
     "start_time": "2024-05-26T09:52:26.704182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Define a custom ensemble model named `VotingModel` that inherits from `BaseEstimator` and `RegressorMixin`.\n",
    "- The `__init__` method initializes the model with a list of estimators (models) to be used for voting aggregation.\n",
    "- The `fit` method is implemented but does nothing, as fitting is not required for voting aggregation.\n",
    "- The `predict` method performs prediction using voting aggregation on the provided features by averaging predictions from all estimators.\n",
    "- The `predict_proba` method performs prediction with probabilities using voting aggregation on the provided features.\n",
    "- For prediction with probabilities, it first collects predictions from LightGBM (lgb) models and then from categorical (cat) models. Categorical columns are converted to string type before making predictions to ensure compatibility.\n",
    "- Finally, the predictions are averaged across all models to get the final predicted probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78f39518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:26.755371Z",
     "iopub.status.busy": "2024-05-26T09:52:26.755075Z",
     "iopub.status.idle": "2024-05-26T09:52:26.764964Z",
     "shell.execute_reply": "2024-05-26T09:52:26.764114Z"
    },
    "papermill": {
     "duration": 0.029061,
     "end_time": "2024-05-26T09:52:26.766772",
     "exception": false,
     "start_time": "2024-05-26T09:52:26.737711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom ensemble model that performs voting aggregation for predictions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    estimators : list\n",
    "        List of estimators (models) to be used for voting aggregation.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None):\n",
    "        Fit the ensemble model. This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "    predict(X):\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "    predict_proba(X):\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators):\n",
    "        \"\"\"\n",
    "        Initialize the VotingModel.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        estimators : list\n",
    "            List of estimators (models) to be used for voting aggregation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the ensemble model.\n",
    "\n",
    "        This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,) (default=None)\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array-like, shape (n_samples,)\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "     \n",
    "    def predict_proba(self, X):      \n",
    "        \"\"\"\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred_proba : array-like, shape (n_samples, n_classes)\n",
    "            Predicted probabilities.\n",
    "        \"\"\"\n",
    "        # lgb\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators[:5]]\n",
    "        \n",
    "        # cat\n",
    "        X[cat_cols] = X[cat_cols].astype(str)\n",
    "        y_preds += [estimator.predict_proba(X) for estimator in self.estimators[-5:]]\n",
    "        \n",
    "        return np.mean(y_preds, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99eb4515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:26.802037Z",
     "iopub.status.busy": "2024-05-26T09:52:26.801746Z",
     "iopub.status.idle": "2024-05-26T09:52:26.807647Z",
     "shell.execute_reply": "2024-05-26T09:52:26.806854Z"
    },
    "papermill": {
     "duration": 0.025768,
     "end_time": "2024-05-26T09:52:26.809577",
     "exception": false,
     "start_time": "2024-05-26T09:52:26.783809",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VotingModel(lgb_models + cat_models)\n",
    "len(model.estimators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c2473421",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:26.845622Z",
     "iopub.status.busy": "2024-05-26T09:52:26.845377Z",
     "iopub.status.idle": "2024-05-26T09:52:27.938306Z",
     "shell.execute_reply": "2024-05-26T09:52:27.937510Z"
    },
    "papermill": {
     "duration": 1.113847,
     "end_time": "2024-05-26T09:52:27.940568",
     "exception": false,
     "start_time": "2024-05-26T09:52:26.826721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = pd.Series(model.predict_proba(df_test)[:, 1], index=df_test.index)\n",
    "df_subm = pd.read_csv(ROOT / \"sample_submission.csv\")\n",
    "df_subm = df_subm.set_index(\"case_id\")\n",
    "df_subm[\"score\"] = y_pred\n",
    "df_subm.to_csv(\"sub.csv\")\n",
    "df_train,y,df_test=joblib.load('/kaggle/working/data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7b64ce",
   "metadata": {
    "papermill": {
     "duration": 0.017671,
     "end_time": "2024-05-26T09:52:27.976248",
     "exception": false,
     "start_time": "2024-05-26T09:52:27.958577",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4c68f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:52:28.013307Z",
     "iopub.status.busy": "2024-05-26T09:52:28.012972Z",
     "iopub.status.idle": "2024-05-26T09:54:10.913648Z",
     "shell.execute_reply": "2024-05-26T09:54:10.912553Z"
    },
    "papermill": {
     "duration": 102.922226,
     "end_time": "2024-05-26T09:54:10.916380",
     "exception": false,
     "start_time": "2024-05-26T09:52:27.994154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10, number of negative: 1526659\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.174565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 46168\n",
      "[LightGBM] [Info] Number of data points in the train set: 1526669, number of used features: 305\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.000007 -> initscore=-11.936007\n",
      "[LightGBM] [Info] Start training from score -11.936007\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    }
   ],
   "source": [
    "fitted_models_lgb=[]\n",
    "model = lgb.LGBMClassifier()\n",
    "model.fit(df_train,y)\n",
    "fitted_models_lgb.append(model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b90623",
   "metadata": {
    "papermill": {
     "duration": 0.020049,
     "end_time": "2024-05-26T09:54:10.958309",
     "exception": false,
     "start_time": "2024-05-26T09:54:10.938260",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explanation:**\n",
    "- Defines a custom ensemble model named `VotingModel` for voting aggregation of predictions.\n",
    "- It inherits from `BaseEstimator` and `RegressorMixin`.\n",
    "- The `__init__` method initializes the model with a list of fitted estimators (models) to be used for voting aggregation.\n",
    "- The `fit` method is implemented but does nothing, as fitting is not required for voting aggregation.\n",
    "- The `predict` method performs prediction using voting aggregation on the provided features by averaging predictions from all fitted estimators.\n",
    "- The `predict_proba` method performs prediction with probabilities using voting aggregation on the provided features by averaging probabilities from all fitted estimators.\n",
    "- An instance of `VotingModel` is then created with a list of fitted LightGBM models (`fitted_models_lgb`).\n",
    "\n",
    "This custom ensemble model allows for easy integration of multiple fitted models for voting aggregation of predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c116aa43",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:54:11.000626Z",
     "iopub.status.busy": "2024-05-26T09:54:11.000265Z",
     "iopub.status.idle": "2024-05-26T09:54:11.010168Z",
     "shell.execute_reply": "2024-05-26T09:54:11.009412Z"
    },
    "papermill": {
     "duration": 0.033529,
     "end_time": "2024-05-26T09:54:11.012169",
     "exception": false,
     "start_time": "2024-05-26T09:54:10.978640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VotingModel(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    A custom ensemble model for voting aggregation of predictions.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    estimators : list\n",
    "        List of fitted estimators (models) to be used for voting aggregation.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    fit(X, y=None):\n",
    "        Fit the ensemble model. This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "    predict(X):\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "    predict_proba(X):\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, estimators):\n",
    "        \"\"\"\n",
    "        Initialize the VotingModel.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        estimators : list\n",
    "            List of fitted estimators (models) to be used for voting aggregation.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.estimators = estimators\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit the ensemble model.\n",
    "\n",
    "        This method does nothing as it's not required for voting aggregation.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Training data.\n",
    "        y : array-like, shape (n_samples,) (default=None)\n",
    "            Target values.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform prediction using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred : array-like, shape (n_samples,)\n",
    "            Predicted target values.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Perform prediction with probabilities using voting aggregation on the provided features.\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape (n_samples, n_features)\n",
    "            Features to perform prediction on.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        y_pred_proba : array-like, shape (n_samples, n_classes)\n",
    "            Predicted probabilities.\n",
    "        \"\"\"\n",
    "        y_preds = [estimator.predict_proba(X) for estimator in self.estimators]\n",
    "        return np.mean(y_preds, axis=0)\n",
    "\n",
    "model = VotingModel(fitted_models_lgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4d4ae",
   "metadata": {
    "papermill": {
     "duration": 0.018809,
     "end_time": "2024-05-26T09:54:11.049864",
     "exception": false,
     "start_time": "2024-05-26T09:54:11.031055",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Processing and Model Prediction in Chunks\n",
    "\n",
    "### **Explanation**\n",
    "This code processes a DataFrame (`df_test`) in chunks, making predictions using a pre-trained model and updating a submission file accordingly. The processing in chunks helps manage memory usage efficiently, especially with large datasets. \n",
    "\n",
    "1. **Dropping Unnecessary Columns**: The columns `WEEK_NUM` and `target` are removed from `df_test` as they are not needed for predictions.\n",
    "2. **Setting Index**: The index of `df_test` is set to `case_id` for easier lookup and alignment with other dataframes.\n",
    "3. **Defining Chunk Size**: The size of each chunk to be processed is set to 200 rows.\n",
    "4. **Creating an Empty DataFrame**: An empty DataFrame `df_subm_final` is initialized to store the final processed data.\n",
    "5. **Processing in Chunks**:\n",
    "   - A loop iterates over `df_test` in chunks of 200 rows.\n",
    "   - For each chunk, predictions are made using a model's `predict_proba` method.\n",
    "   - A condition is applied to update scores based on the prediction probabilities.\n",
    "   - The corresponding chunk from a submission file (`sub.csv`) is read and updated.\n",
    "   - The processed chunk is appended to `df_subm_final`.\n",
    "6. **Saving the Final Submission File**: The final DataFrame `df_subm_final` is saved to `submission.csv`.\n",
    "7. **Cleanup**: A temporary file (`data.pkl`) is removed to free up space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e84ab03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-26T09:54:11.089009Z",
     "iopub.status.busy": "2024-05-26T09:54:11.088703Z",
     "iopub.status.idle": "2024-05-26T09:54:12.384095Z",
     "shell.execute_reply": "2024-05-26T09:54:12.382719Z"
    },
    "papermill": {
     "duration": 1.318252,
     "end_time": "2024-05-26T09:54:12.386883",
     "exception": false,
     "start_time": "2024-05-26T09:54:11.068631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "df_test = df_test.drop(columns=[\"WEEK_NUM\", 'target'])\n",
    "# Setting the index to 'case_id'\n",
    "df_test = df_test.set_index(\"case_id\")\n",
    "\n",
    "# Define chunk size for processing\n",
    "chunk_size = 200\n",
    "\n",
    "# Initialize an empty DataFrame to store final results\n",
    "df_subm_final = pd.DataFrame()\n",
    "\n",
    "# Process df_test in chunks\n",
    "for chunk_start in range(0, len(df_test), chunk_size):\n",
    "    # Select a chunk of df_test\n",
    "    df_test_chunk = df_test.iloc[chunk_start:chunk_start + chunk_size]\n",
    "    \n",
    "    # Make predictions on the current chunk using the pre-trained model\n",
    "    y_pred_chunk = pd.Series(model.predict_proba(df_test_chunk)[:, 1], index=df_test_chunk.index)\n",
    "    \n",
    "    # Apply condition to filter predictions below a certain threshold (0.985)\n",
    "    condition_chunk = y_pred_chunk < 0.985\n",
    "    \n",
    "    # Read corresponding part of the submission file\n",
    "    df_subm_chunk = pd.read_csv(\"/kaggle/working/sub.csv\", \n",
    "                                skiprows=range(1, chunk_start + 1), \n",
    "                                nrows=chunk_size, \n",
    "                                index_col=\"case_id\", \n",
    "                                header=0)\n",
    "    \n",
    "    # Update scores in the submission chunk based on condition\n",
    "    df_subm_chunk.loc[condition_chunk, 'score'] = (df_subm_chunk.loc[condition_chunk, 'score'] - 0.0735).clip(0)\n",
    "    \n",
    "    # Append the processed chunk to the final submission DataFrame\n",
    "    df_subm_final = pd.concat([df_subm_final, df_subm_chunk])\n",
    "\n",
    "# Save the final submission file to 'submission.csv'\n",
    "df_subm_final.to_csv(\"submission.csv\")\n",
    "\n",
    "# Remove temporary file if necessary\n",
    "!rm -rf data.pkl\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 7921029,
     "sourceId": 50160,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 27710,
     "sourceId": 33095,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 27711,
     "sourceId": 33096,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 279.713256,
   "end_time": "2024-05-26T09:54:13.367715",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-26T09:49:33.654459",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
